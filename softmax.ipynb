{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "softmax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1012598167/cs231n/blob/main/softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRK3KHUgzZVM",
        "outputId": "82bc33a1-d5b8-40f1-8cab-0de340a5adbf"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 输入daseCV所在的路径\n",
        "# 'daseCV' 文件夹包括 '.py', 'classifiers' 和'datasets'文件夹\n",
        "# 例如 'CV/assignments/assignment1/daseCV/'\n",
        "FOLDERNAME = 'CV/assignments/assignment1/daseCV/'\n",
        "\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "%cd drive/My\\ Drive\n",
        "%cp -r $FOLDERNAME ../../\n",
        "%cd ../../\n",
        "%cd daseCV/datasets/\n",
        "!bash get_datasets.sh\n",
        "%cd ../../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n",
            "/content\n",
            "/content/daseCV/datasets\n",
            "--2021-04-03 12:56:42--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  70.4MB/s    in 2.3s    \n",
            "\n",
            "2021-04-03 12:56:45 (70.4 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKAUUtV6zZVU"
      },
      "source": [
        "# Softmax 练习\n",
        "\n",
        "*补充并完成本练习。*\n",
        "\n",
        "本练习类似于SVM练习，你要完成的事情包括:\n",
        "\n",
        "- 为Softmax分类器实现完全矢量化的**损失函数**\n",
        "- 实现其**解析梯度（analytic gradient）**的完全矢量化表达式\n",
        "- 用数值梯度**检查你的代码**\n",
        "- 使用验证集**调整学习率和正则化强度**\n",
        "- 使用**SGD优化**损失函数\n",
        "- **可视化**最终学习的权重\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EaUEHP0zZVV",
        "outputId": "aeec5078-bcb0-4983-ef56-b213cdc410f6"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from daseCV.data_utils import load_CIFAR10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading extenrnal modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4C0rBaezZVV",
        "outputId": "46e0605a-b26b-4f3f-edf9-79d265098237"
      },
      "source": [
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for the linear classifier. These are the same steps as we used for the\n",
        "    SVM, but condensed to a single function.  \n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = 'daseCV/datasets/cifar-10-batches-py'\n",
        "    \n",
        "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
        "    try:\n",
        "       del X_train, y_train\n",
        "       del X_test, y_test\n",
        "       print('Clear previously loaded data.')\n",
        "    except:\n",
        "       pass\n",
        "\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "    \n",
        "    # subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "    X_dev = X_train[mask]\n",
        "    y_dev = y_train[mask]\n",
        "    \n",
        "    # Preprocessing: reshape the image data into rows\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
        "    \n",
        "    # Normalize the data: subtract the mean image\n",
        "    mean_image = np.mean(X_train, axis = 0)\n",
        "    X_train -= mean_image\n",
        "    X_val -= mean_image\n",
        "    X_test -= mean_image\n",
        "    X_dev -= mean_image\n",
        "    \n",
        "    # add bias dimension and transform into columns\n",
        "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
        "\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)\n",
        "print('dev data shape: ', X_dev.shape)\n",
        "print('dev labels shape: ', y_dev.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape:  (49000, 3073)\n",
            "Train labels shape:  (49000,)\n",
            "Validation data shape:  (1000, 3073)\n",
            "Validation labels shape:  (1000,)\n",
            "Test data shape:  (1000, 3073)\n",
            "Test labels shape:  (1000,)\n",
            "dev data shape:  (500, 3073)\n",
            "dev labels shape:  (500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLJNAlLFzZVW"
      },
      "source": [
        "## Softmax 分类器\n",
        "\n",
        "请在**daseCV/classifiers/softmax.py**中完成本节的代码。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X7KyatKzZVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82afdff5-f3d5-4a5c-ae34-aef4b08c2ac2"
      },
      "source": [
        "# 首先使用嵌套循环实现简单的softmax损失函数。\n",
        "# 打开文件 daseCV/classifiers/softmax.py 并补充完成\n",
        "# softmax_loss_naive 函数.\n",
        "\n",
        "from daseCV.classifiers.softmax import softmax_loss_naive\n",
        "import time\n",
        "\n",
        "# 生成一个随机的softmax权重矩阵，并使用它来计算损失。\n",
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
        "print('loss: %f' % loss)\n",
        "print('sanity check: %f' % (-np.log(0.1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 2.489394\n",
            "sanity check: 2.302585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI7UC1el0pMK",
        "outputId": "0f4607ed-daf6-45e7-9e3a-71e89393e784"
      },
      "source": [
        "W[1:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-5.63857262e-05,  1.42104178e-04, -1.63497641e-04,\n",
              "         5.21989270e-05, -1.01614662e-04,  3.17728984e-05,\n",
              "         1.08077893e-04, -2.56450380e-05, -2.30000045e-04,\n",
              "         1.67030872e-05],\n",
              "       [ 3.25436775e-05,  7.71187498e-06, -6.35514754e-05,\n",
              "         2.31037970e-05, -3.83040886e-05, -1.77027887e-04,\n",
              "         2.10226448e-05,  2.68918820e-05, -1.94214241e-05,\n",
              "        -2.90174960e-05],\n",
              "       [ 5.37011753e-05, -1.60069226e-04,  8.96287286e-05,\n",
              "         1.82900324e-04,  1.85280903e-04, -1.11433733e-04,\n",
              "        -1.67590517e-05, -7.13481146e-05,  4.07533752e-05,\n",
              "         3.76939953e-05]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgF7-jApzZVX"
      },
      "source": [
        "**问题 1**\n",
        "\n",
        "\n",
        "为什么我们期望损失接近-log（0.1）？简要说明。\n",
        "\n",
        "$\\color{blue}{\\textit 答:}$ 由于权重矩阵W是均匀随机选择的，因此每个类别的预测概率是均匀分布，并且等于1/10，其中10是类别数。 因此，每个示例的交叉熵是-log（0.1），应等于损失。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTUw2HXgzZVX",
        "outputId": "f1077a03-e537-4444-bccd-f97012f5ae73"
      },
      "source": [
        "# 完成softmax_loss_naive，并实现使用嵌套循环的梯度的版本(naive)。\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# 就像SVM那样，请使用数值梯度检查作为调试工具。\n",
        "# 数值梯度应接近分析梯度。\n",
        "from daseCV.gradient_check import grad_check_sparse\n",
        "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
        "\n",
        "# 与SVM情况类似，使用正则化进行另一个梯度检查\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
        "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numerical: 1.957841 analytic: 1.957841, relative error: 6.166928e-08\n",
            "numerical: -5.867751 analytic: -5.867751, relative error: 3.358170e-09\n",
            "numerical: -0.634055 analytic: -0.634055, relative error: 1.251246e-08\n",
            "numerical: 3.583884 analytic: 3.583884, relative error: 1.469139e-08\n",
            "numerical: -0.694063 analytic: -0.694063, relative error: 5.483703e-08\n",
            "numerical: 0.402959 analytic: 0.402959, relative error: 4.802526e-08\n",
            "numerical: 0.306422 analytic: 0.306422, relative error: 4.179119e-08\n",
            "numerical: -0.759939 analytic: -0.759940, relative error: 9.215787e-08\n",
            "numerical: -1.202137 analytic: -1.202137, relative error: 2.961540e-08\n",
            "numerical: 0.838005 analytic: 0.838005, relative error: 7.304510e-09\n",
            "numerical: 0.499000 analytic: 0.499000, relative error: 2.633208e-08\n",
            "numerical: 1.361596 analytic: 1.361596, relative error: 3.284206e-08\n",
            "numerical: -3.165223 analytic: -3.165223, relative error: 1.507749e-08\n",
            "numerical: 4.474712 analytic: 4.474712, relative error: 1.334087e-08\n",
            "numerical: -0.208245 analytic: -0.208245, relative error: 2.036710e-07\n",
            "numerical: 0.459994 analytic: 0.459994, relative error: 3.246983e-08\n",
            "numerical: 0.946844 analytic: 0.946844, relative error: 4.074694e-08\n",
            "numerical: 0.986594 analytic: 0.986594, relative error: 3.401871e-08\n",
            "numerical: -2.064113 analytic: -2.064113, relative error: 1.278162e-08\n",
            "numerical: -0.691414 analytic: -0.691415, relative error: 1.335196e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Mtq55BHzZVY",
        "outputId": "30b1deb6-a672-4a59-c592-0163aef50748"
      },
      "source": [
        "# 现在，我们有了softmax损失函数及其梯度的简单实现，\n",
        "# 接下来要在 softmax_loss_vectorized 中完成一个向量化版本.\n",
        "# 这两个版本应计算出相同的结果，但矢量化版本应更快。\n",
        "tic = time.time()\n",
        "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
        "\n",
        "from daseCV.classifiers.softmax import softmax_loss_vectorized\n",
        "tic = time.time()\n",
        "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
        "\n",
        "# 正如前面在SVM练习中所做的一样，我们使用Frobenius范数比较两个版本梯度。\n",
        "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
        "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
        "print('Gradient difference: %f' % grad_difference)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naive loss: 2.489394e+00 computed in 0.582451s\n",
            "vectorized loss: 2.489394e+00 computed in 0.019241s\n",
            "Loss difference: 0.000000\n",
            "Gradient difference: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2mxw2qizZVY",
        "outputId": "7663c911-06d8-4c9c-d5bd-8043a6b64b24"
      },
      "source": [
        "# 使用验证集调整超参数（正则化强度和学习率）。您应该尝试不同的学习率和正则化强度范围; \n",
        "# 如果您小心的话，您应该能够在验证集上获得超过0.35的精度。\n",
        "from daseCV.classifiers import Softmax\n",
        "results = {}\n",
        "best_val = -1\n",
        "best_softmax = None\n",
        "learning_rates = [1e-7, 2e-7, 5e-7]\n",
        "#regularization_strengths = [5e4, 1e8]\n",
        "regularization_strengths =[(1+0.1*i)*1e4 for i in range(-3,4)] + [(5+0.1*i)*1e4 for i in range(-3,4)]\n",
        "\n",
        "################################################################################\n",
        "# 需要完成的事:                                                                        \n",
        "# 对验证集设置学习率和正则化强度。\n",
        "# 这与之前SVM中做的类似；\n",
        "# 保存训练效果最好的softmax分类器到best_softmax中。\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for rs in regularization_strengths:\n",
        "        softmax = Softmax()\n",
        "        softmax.train(X_train, y_train, lr, rs, num_iters=2000)\n",
        "        y_train_pred = softmax.predict(X_train)\n",
        "        train_accuracy = np.mean(y_train == y_train_pred)\n",
        "        y_val_pred = softmax.predict(X_val)\n",
        "        val_accuracy = np.mean(y_val == y_val_pred)\n",
        "        if val_accuracy > best_val:\n",
        "            best_val = val_accuracy\n",
        "            best_softmax = softmax           \n",
        "        results[(lr,rs)] = train_accuracy, val_accuracy\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    \n",
        "# Print out results.\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "    \n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr 1.000000e-07 reg 7.000000e+03 train accuracy: 0.339367 val accuracy: 0.344000\n",
            "lr 1.000000e-07 reg 8.000000e+03 train accuracy: 0.350000 val accuracy: 0.335000\n",
            "lr 1.000000e-07 reg 9.000000e+03 train accuracy: 0.356163 val accuracy: 0.363000\n",
            "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.359980 val accuracy: 0.358000\n",
            "lr 1.000000e-07 reg 1.100000e+04 train accuracy: 0.358694 val accuracy: 0.364000\n",
            "lr 1.000000e-07 reg 1.200000e+04 train accuracy: 0.358816 val accuracy: 0.367000\n",
            "lr 1.000000e-07 reg 1.300000e+04 train accuracy: 0.364306 val accuracy: 0.367000\n",
            "lr 1.000000e-07 reg 4.700000e+04 train accuracy: 0.331286 val accuracy: 0.345000\n",
            "lr 1.000000e-07 reg 4.800000e+04 train accuracy: 0.331898 val accuracy: 0.352000\n",
            "lr 1.000000e-07 reg 4.900000e+04 train accuracy: 0.333061 val accuracy: 0.359000\n",
            "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.328306 val accuracy: 0.338000\n",
            "lr 1.000000e-07 reg 5.100000e+04 train accuracy: 0.328204 val accuracy: 0.348000\n",
            "lr 1.000000e-07 reg 5.200000e+04 train accuracy: 0.327612 val accuracy: 0.347000\n",
            "lr 1.000000e-07 reg 5.300000e+04 train accuracy: 0.321837 val accuracy: 0.339000\n",
            "lr 2.000000e-07 reg 7.000000e+03 train accuracy: 0.377163 val accuracy: 0.395000\n",
            "lr 2.000000e-07 reg 8.000000e+03 train accuracy: 0.377694 val accuracy: 0.388000\n",
            "lr 2.000000e-07 reg 9.000000e+03 train accuracy: 0.374469 val accuracy: 0.386000\n",
            "lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.371918 val accuracy: 0.394000\n",
            "lr 2.000000e-07 reg 1.100000e+04 train accuracy: 0.371776 val accuracy: 0.386000\n",
            "lr 2.000000e-07 reg 1.200000e+04 train accuracy: 0.368755 val accuracy: 0.385000\n",
            "lr 2.000000e-07 reg 1.300000e+04 train accuracy: 0.371122 val accuracy: 0.382000\n",
            "lr 2.000000e-07 reg 4.700000e+04 train accuracy: 0.331286 val accuracy: 0.351000\n",
            "lr 2.000000e-07 reg 4.800000e+04 train accuracy: 0.325878 val accuracy: 0.336000\n",
            "lr 2.000000e-07 reg 4.900000e+04 train accuracy: 0.332143 val accuracy: 0.347000\n",
            "lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.331612 val accuracy: 0.341000\n",
            "lr 2.000000e-07 reg 5.100000e+04 train accuracy: 0.323061 val accuracy: 0.338000\n",
            "lr 2.000000e-07 reg 5.200000e+04 train accuracy: 0.327286 val accuracy: 0.340000\n",
            "lr 2.000000e-07 reg 5.300000e+04 train accuracy: 0.321673 val accuracy: 0.339000\n",
            "lr 5.000000e-07 reg 7.000000e+03 train accuracy: 0.379857 val accuracy: 0.388000\n",
            "lr 5.000000e-07 reg 8.000000e+03 train accuracy: 0.378265 val accuracy: 0.391000\n",
            "lr 5.000000e-07 reg 9.000000e+03 train accuracy: 0.374265 val accuracy: 0.378000\n",
            "lr 5.000000e-07 reg 1.000000e+04 train accuracy: 0.369388 val accuracy: 0.371000\n",
            "lr 5.000000e-07 reg 1.100000e+04 train accuracy: 0.372449 val accuracy: 0.385000\n",
            "lr 5.000000e-07 reg 1.200000e+04 train accuracy: 0.366245 val accuracy: 0.370000\n",
            "lr 5.000000e-07 reg 1.300000e+04 train accuracy: 0.363592 val accuracy: 0.379000\n",
            "lr 5.000000e-07 reg 4.700000e+04 train accuracy: 0.332469 val accuracy: 0.345000\n",
            "lr 5.000000e-07 reg 4.800000e+04 train accuracy: 0.325755 val accuracy: 0.341000\n",
            "lr 5.000000e-07 reg 4.900000e+04 train accuracy: 0.330694 val accuracy: 0.335000\n",
            "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.330122 val accuracy: 0.337000\n",
            "lr 5.000000e-07 reg 5.100000e+04 train accuracy: 0.329408 val accuracy: 0.339000\n",
            "lr 5.000000e-07 reg 5.200000e+04 train accuracy: 0.327898 val accuracy: 0.342000\n",
            "lr 5.000000e-07 reg 5.300000e+04 train accuracy: 0.320122 val accuracy: 0.337000\n",
            "best validation accuracy achieved during cross-validation: 0.395000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ifq-BXseRCGu",
        "outputId": "4dd0b979-d4fb-49b5-d093-de626863924c"
      },
      "source": [
        "print(\"softmax # 在测试集上评估\n",
        "# 在测试集上评估最好的softmax\n",
        "y_test_pred = best_softmax.predict(X_test)\n",
        "test_accuracy = np.mean(y_test == y_test_pred)\n",
        "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))on raw pixels final test set accuracy: 0.369000\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax on raw pixels final test set accuracy: 0.369000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmt3YjcYzZVZ"
      },
      "source": [
        "**问题 2** - *对或错*\n",
        "\n",
        "假设总训练损失定义为所有训练样本中每个数据点损失的总和。可能会有新的数据点添加到训练集中，同时SVM损失保持不变，但是对于Softmax分类器的损失而言，情况并非如此。\n",
        "\n",
        "$\\color{blue}{\\textit 你的回答:}$对\n",
        "\n",
        "\n",
        "$\\color{blue}{\\textit 你的解释:}$softmax值都大于0的 怎么加新点都会变大\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEHi-ouIRol6"
      },
      "source": [
        "# 可视化每个类别的学习到的权重\n",
        "w = best_softmax.W[:-1,:] # strip out the bias\n",
        "w = w.reshape(32, 32, 3, 10)\n",
        "\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    \n",
        "    # Rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCcEqJRhzZVa"
      },
      "source": [
        "---\n",
        "# 重要\n",
        "\n",
        "这里是作业的结尾处，请执行以下步骤:\n",
        "\n",
        "1. 点击`File -> Save`或者用`control+s`组合键，确保你最新的的notebook的作业已经保存到谷歌云。\n",
        "2. 执行以下代码确保 `.py` 文件保存回你的谷歌云。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXzwZbZjS5vO"
      },
      "source": [
        "import os\n",
        "\n",
        "FOLDER_TO_SAVE = os.path.join('drive/My Drive/', FOLDERNAME)\n",
        "FILES_TO_SAVE = ['daseCV/classifiers/softmax.py']\n",
        "\n",
        "for files in FILES_TO_SAVE:\n",
        "  with open(os.path.join(FOLDER_TO_SAVE, '/'.join(files.split('/')[1:])), 'w') as f:\n",
        "    f.write(''.join(open(files).readlines()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}